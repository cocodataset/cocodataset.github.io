<div class="columnApi">
  <p class="fontTitle">Tools</p>
  <p class="fontSubtitle"><a href="https://github.com/pdollar/coco" target="_blank">COCO API</a></p>
</div>
<div class="columnDownloads">
  <p class="fontTitle">Images</p>
  <p class="fontSmall">
    <a href="http://images.cocodataset.org/zips/train2014.zip">2014 Train images [83K/13GB]</a><br/>
    <a href="http://images.cocodataset.org/zips/val2014.zip">2014 Val images [41K/6GB]</a><br/>
    <a href="http://images.cocodataset.org/zips/test2014.zip">2014 Test images [41K/6GB]</a><br/>
    <a href="http://images.cocodataset.org/zips/test2015.zip">2015 Test images [81K/12GB]</a><br/>
    <a href="http://images.cocodataset.org/zips/train2017.zip">2017 Train images [118K/18GB]</a><br/>
    <a href="http://images.cocodataset.org/zips/val2017.zip">2017 Val images [5K/1GB]</a><br/>
    <a href="http://images.cocodataset.org/zips/test2017.zip">2017 Test images [41K/6GB]</a><br/>
    <a href="http://images.cocodataset.org/zips/unlabeled2017.zip">2017 Unlabeled images [123K/19GB]</a><br/>
  </p>
</div>
<div class="columnDownloads">
  <p class="fontTitle">Annotations</p>
  <p class="fontSmall">
    <a href="http://images.cocodataset.org/annotations/annotations_trainval2014.zip">2014 Train/Val annotations [241MB]</a><br/>
    <a href="http://images.cocodataset.org/annotations/image_info_test2014.zip">2014 Testing Image info [1MB]</a><br/>
    <a href="http://images.cocodataset.org/annotations/image_info_test2015.zip">2015 Testing Image info [2MB]</a><br/>
    <a href="http://images.cocodataset.org/annotations/annotations_trainval2017.zip">2017 Train/Val annotations [241MB]</a><br/>
    <a href="http://images.cocodataset.org/annotations/stuff_annotations_trainval2017.zip">2017 Stuff Train/Val annotations [191MB]</a><br/>
    <a href="http://images.cocodataset.org/annotations/image_info_test2017.zip">2017 Testing Image info [1MB]</a><br/>
    <a href="http://images.cocodataset.org/annotations/image_info_unlabeled2017.zip">2017 Unlabeled Image info [4MB]</a><br/>
  </p>
</div>
<p class="fontTitle">1. Overview</p>

<p>Which dataset splits should you download? Each year's split is associated with different challenges. Specifically:</p>
<div class="json">
  <div class="jsonktxt">2014 Train/Val</div><div class="jsonvtxt"><a href="#detections-challenge2015">Detection 2015</a>, <a href="#captions-challenge2015">Captioning 2015</a>, <a href="#detections-challenge2016">Detection 2016</a>, <a href="#keypoints-challenge2016">Keypoints 2016</a></div>
  <div class="jsonktxt">2014 Testing  </div><div class="jsonvtxt"><a href="#captions-challenge2015">Captioning 2015</a></div>
  <div class="jsonktxt">2015 Testing  </div><div class="jsonvtxt"><a href="#detections-challenge2015">Detection 2015</a>, <a href="#detections-challenge2016">Detection 2016</a>, <a href="#keypoints-challenge2016">Keypoints 2016</a>, <a href="#stuff-challenge2017">Stuff 2017</a></div>
  <div class="jsonktxt">2017 Train/Val</div><div class="jsonvtxt"><a href="#detections-challenge2017">Detection 2017</a>, <a href="#keypoints-challenge2017">Keypoints 2017</a>, <a href="#stuff-challenge2017">Stuff 2017</a></div>
  <div class="jsonktxt">2017 Testing  </div><div class="jsonvtxt"><a href="#detections-challenge2017">Detection 2017</a>, <a href="#keypoints-challenge2017">Keypoints 2017</a></div>
  <div class="jsonktxt">2017 Unlabeled</div><div class="jsonvtxt fontBlue">[optional data for any competition]</div>
</div>
<p class="fontSubtle">If you are submitting to a 2017 challenge, you only need to download the 2017 data. You can disregard earlier data splits.</p>
<p>For efficiently downloading the images, we recommend using <a href="#gsutil-rsync" data-toggle="collapse" class="fontBold"><span class="glyphicon glyphicon-expand"></span> gsutil rsync</a> to avoid the download of large zip files.</p>
<div id="gsutil-rsync" class="collapse">
  <div class="json">
    <p>Our data is hosted on Google Cloud Platform (GCP). <span class="fontMono"> gsutil</span> provides tools for efficiently accessing this data. You do <i>not</i> need a GCP account to use <span class="fontMono"> gsutil</span>. Instructions for downloading the data are as follows:</p>
    <div class="jsonktxt">(1) Install gsutil via:</div><div class="jsonvtxt fontBlue">curl https://sdk.cloud.google.com | bash </div>
    <div class="jsonktxt">(2) Make local dir:</div><div class="jsonvtxt fontBlue">mkdir val2017</div>
    <div class="jsonktxt">(3) Synchronize via:</div><div class="jsonvtxt fontBlue">gsutil -m rsync gs://images.cocodataset.org/val2017 val2017</div>
    <p>The splits are available for download via rsync are: train2014, val2014, test2014, test2015, train2017, val2017, test2017, unlabeled2017. Simply replace 'val2017' with the split you wish to download and repeat steps (2)-(3). Finally, you can also download all the annotation zip files via:</p>
    <div class="jsonktxt">(4) Get annotations: </div><div class="jsonvtxt fontBlue">gsutil -m rsync gs://images.cocodataset.org/annotations [localdir]</div>
    <p>The download is multi-threaded, you can control other options of the download as well (see <a href="https://cloud.google.com/storage/docs/gsutil/commands/rsync">gsutil rsync</a>). Please do not contact us with help
    <a href="https://cloud.google.com/storage/docs/gsutil_install">installing gsutil</a> (we note only that you do not need to run <span class="fontMono"> gcloud init</span>).</p>
  </div>
</div>
<p>Please follow the instructions in the <a href="https://github.com/pdollar/coco" target="_blank">COCO API Readme</a> to setup the downloaded COCO data (the images and annotations).
By downloading this dataset, you agree to our <a href="termsofuse.htm">Terms of Use</a>.</p>
<p><b>2017 Update:</b> The main change in 2017 is that instead of an 80K/40K train/val split, based on community feedback the split is now 115K/5K for train/val. The same exact images are used, and no new annotations for detection/keypoints are provided. However, new in 2017 are stuff annotations on 40K train images (subset of the full 115K train images from 2017) and 5K val images. Also, for testing, in 2017 the test set only has two splits (dev / challenge), instead of the four splits (dev / standard / reserve / challenge) used in previous years. Finally, new in 2017 we are releasing 120K unlabeled images from COCO that follow the same class distribution as the labeled images; this may be useful for semi-supervised learning on COCO.</p>
<p><b>Note:</b> Annotations last updated <i>09/05/2017</i> (stuff annotations added). If you find any issues with the data please let us know!</p>

<p class="fontTitle">2. COCO API</p>
<p>The COCO API assists in loading, parsing, and visualizing annotations in COCO. The API supports object instance, object keypoint, and image caption annotations (for captions not all functionality is defined). For additional details see: <a href="https://github.com/pdollar/coco/blob/master/MatlabAPI/CocoApi.m" target="_blank">CocoApi.m</a>, <a href="https://github.com/pdollar/coco/blob/master/PythonAPI/pycocotools/coco.py" target="_blank">coco.py</a>, and <a href="https://github.com/pdollar/coco/blob/master/LuaAPI/CocoApi.lua" target="_blank">CocoApi.lua</a> for Matlab, Python, and Lua code, respectively, and also the <a href="https://github.com/pdollar/coco/blob/master/PythonAPI/pycocoDemo.ipynb" target="_blank">Python API demo</a>.</p>
<div class="json">
  <div>Throughout the API "ann"=annotation, "cat"=category, and "img"=image.</div>
  <div class="jsonkfunc">download</div><div class="jsonvtxt">Download COCO images from mscoco.org server.</div>
  <div class="jsonkfunc">getAnnIds</div><div class="jsonvtxt">Get ann ids that satisfy given filter conditions.</div>
  <div class="jsonkfunc">getCatIds</div><div class="jsonvtxt">Get cat ids that satisfy given filter conditions.</div>
  <div class="jsonkfunc">getImgIds</div><div class="jsonvtxt">Get img ids that satisfy given filter conditions.</div>
  <div class="jsonkfunc">loadAnns</div><div class="jsonvtxt">Load anns with the specified ids.</div>
  <div class="jsonkfunc">loadCats</div><div class="jsonvtxt">Load cats with the specified ids.</div>
  <div class="jsonkfunc">loadImgs</div><div class="jsonvtxt">Load imgs with the specified ids.</div>
  <div class="jsonkfunc">loadRes</div><div class="jsonvtxt">Load algorithm results and create API for accessing them.</div>
  <div class="jsonkfunc">showAnns</div><div class="jsonvtxt">Display the specified annotations.</div>
</div>

<p class="fontTitle">3. MASK API</p>
<p>COCO provides segmentation masks for every object instance. This creates two challenges: storing masks compactly and performing mask computations efficiently. We solve both challenges using a custom Run Length Encoding (RLE) scheme. The size of the RLE representation is proportional to the number of boundaries pixels of a mask and operations such as area, union, or intersection can be computed efficiently directly on the RLE. Specifically, assuming fairly simple shapes, the RLE representation is O(&radic;n) where n is number of pixels in the object, and common computations are likewise O(&radic;n). Naively computing the same operations on the decoded masks (stored as an array) would be O(n).</p>
<p>The MASK API provides an interface for manipulating masks stored in RLE format. The API is defined below, for additional details see: <a href="https://github.com/pdollar/coco/blob/master/MatlabAPI/MaskApi.m" target="_blank">MaskApi.m</a>, <a href="https://github.com/pdollar/coco/blob/master/PythonAPI/pycocotools/mask.py" target="_blank">mask.py</a>, or <a href="https://github.com/pdollar/coco/blob/master/LuaAPI/MaskApi.lua" target="_blank">MaskApi.lua</a>. Finally, we note that a majority of ground truth masks are stored as polygons (which are quite compact), these polygons are converted to RLE when needed.</p>
<div class="json">
  <div class="jsonkfunc">encode</div><div class="jsonvtxt">Encode binary masks using RLE.</div>
  <div class="jsonkfunc">decode</div><div class="jsonvtxt">Decode binary masks encoded via RLE.</div>
  <div class="jsonkfunc">merge</div><div class="jsonvtxt">Compute union or intersection of encoded masks.</div>
  <div class="jsonkfunc">iou</div><div class="jsonvtxt">Compute intersection over union between masks.</div>
  <div class="jsonkfunc">area</div><div class="jsonvtxt">Compute area of encoded masks.</div>
  <div class="jsonkfunc">toBbox</div><div class="jsonvtxt">Get bounding boxes surrounding encoded masks.</div>
  <div class="jsonkfunc">frBbox</div><div class="jsonvtxt">Convert bounding boxes to encoded masks.</div>
  <div class="jsonkfunc">frPoly</div><div class="jsonvtxt">Convert polygon to encoded mask.</div>
</div>

<p class="fontTitle">4. Annotation format</p>
<p>COCO currently has three annotation types: object instances, object keypoints, and image captions. The annotations are stored using the <a href="http://json.org/" target="_blank">JSON</a> file format. All annotations share the basic data structure below:</p>
<div class="json">
  <div class="jsonreg">{</div>
  <div class="jsonk">"info"           </div><div class="jsonv">: info,</div>
  <div class="jsonk">"images"         </div><div class="jsonv">: [image],</div>
  <div class="jsonk">"annotations"    </div><div class="jsonv">: [annotation],</div>
  <div class="jsonk">"licenses"       </div><div class="jsonv">: [license],</div>
  <div class="jsonreg">}</div>
  <br/>
  <div class="jsonreg">info{</div>
  <div class="jsonk">"year"           </div><div class="jsonv">: int,</div>
  <div class="jsonk">"version"        </div><div class="jsonv">: str,</div>
  <div class="jsonk">"description"    </div><div class="jsonv">: str,</div>
  <div class="jsonk">"contributor"    </div><div class="jsonv">: str,</div>
  <div class="jsonk">"url"            </div><div class="jsonv">: str,</div>
  <div class="jsonk">"date_created"   </div><div class="jsonv">: datetime,</div>
  <div class="jsonreg">}</div>
  <br/>
  <div class="jsonreg">image{</div>
  <div class="jsonk">"id"             </div><div class="jsonv">: int,</div>
  <div class="jsonk">"width"          </div><div class="jsonv">: int,</div>
  <div class="jsonk">"height"         </div><div class="jsonv">: int,</div>
  <div class="jsonk">"file_name"      </div><div class="jsonv">: str,</div>
  <div class="jsonk">"license"        </div><div class="jsonv">: int,</div>
  <div class="jsonk">"flickr_url"     </div><div class="jsonv">: str,</div>
  <div class="jsonk">"coco_url"       </div><div class="jsonv">: str,</div>
  <div class="jsonk">"date_captured"  </div><div class="jsonv">: datetime,</div>
  <div class="jsonreg">}</div>
  <br/>
  <div class="jsonreg">license{</div>
  <div class="jsonk">"id"             </div><div class="jsonv">: int,</div>
  <div class="jsonk">"name"           </div><div class="jsonv">: str,</div>
  <div class="jsonk">"url"            </div><div class="jsonv">: str,</div>
  <div class="jsonreg">}</div>
</div>
<p>The data structures specific to the various annotation types are described below.</p>

<p class="fontSubtitle">4.1. Object Instance Annotations</p>
<p>Each instance annotation contains a series of fields, including the category id and segmentation mask of the object. The segmentation format depends on whether the instance represents a single object (iscrowd=0 in which case polygons are used) or a collection of objects (iscrowd=1 in which case RLE is used). Note that a single object (iscrowd=0) may require multiple polygons, for example if occluded. Crowd annotations (iscrowd=1) are used to label large groups of objects (e.g. a crowd of people). In addition, an enclosing bounding box is provided for each object (box coordinates are measured from the top left image corner and are 0-indexed). Finally, the categories field of the annotation structure stores the mapping of category id to category and supercategory names. See also the <a href="#detections-challenge2017">Detection Challenge</a>.</p>
<div class="json">
  <div class="jsonreg">annotation{</div>
  <div class="jsonk">"id"             </div><div class="jsonv">: int,</div>
  <div class="jsonk">"image_id"       </div><div class="jsonv">: int,</div>
  <div class="jsonk">"category_id"    </div><div class="jsonv">: int,</div>
  <div class="jsonk">"segmentation"   </div><div class="jsonv">: RLE or [polygon],</div>
  <div class="jsonk">"area"           </div><div class="jsonv">: float,</div>
  <div class="jsonk">"bbox"           </div><div class="jsonv">: [x,y,width,height],</div>
  <div class="jsonk">"iscrowd"        </div><div class="jsonv">: 0 or 1,</div>
  <div class="jsonreg">}</div>
  <br/>
  <div class="jsonreg">categories[{</div>
  <div class="jsonk">"id"             </div><div class="jsonv">: int,</div>
  <div class="jsonk">"name"           </div><div class="jsonv">: str,</div>
  <div class="jsonk">"supercategory"  </div><div class="jsonv">: str,</div>
  <div class="jsonreg">}]</div>
</div>

<p class="fontSubtitle">4.2. Object Keypoint Annotations</p>
<p>A keypoint annotation contains all the data of the object annotation (including id, bbox, etc.) and two additional fields. First, "keypoints" is a length 3k array where k is the total number of keypoints defined for the category. Each keypoint has a 0-indexed location x,y and a visibility flag v defined as v=0: not labeled (in which case x=y=0), v=1: labeled but not visible, and v=2: labeled and visible. A keypoint is considered visible if it falls inside the object segment. "num_keypoints" indicates the number of labeled keypoints (v>0) for a given object (many objects, e.g. crowds and small objects, will have num_keypoints=0). Finally, for each category, the categories struct has two additional fields: "keypoints," which is a length k array of keypoint names, and "skeleton", which defines connectivity via a list of keypoint edge pairs and is used for visualization. Currently keypoints are only labeled for the person category (for most medium/large non-crowd person instances). See also the <a href="#keypoints-challenge2017">Keypoint Challenge</a>.</p>
<div class="json">
  <div class="jsonreg">annotation{</div>
  <div class="jsonk">"keypoints"        </div><div class="jsonv">: [x1,y1,v1,...],</div>
  <div class="jsonk">"num_keypoints"    </div><div class="jsonv">: int,</div>
  <div class="jsonk">"[cloned]"         </div><div class="jsonv">: ...,</div>
  <div class="jsonreg">}</div>
  <br/>
  <div class="jsonreg">categories[{</div>
  <div class="jsonk">"keypoints"        </div><div class="jsonv">: [str],</div>
  <div class="jsonk">"skeleton"         </div><div class="jsonv">: [edge],</div>
  <div class="jsonk">"[cloned]"         </div><div class="jsonv">: ...,</div>
  <div class="jsonreg">}]</div>
  <br/>
  "[cloned]": denotes fields copied from object instance annotations defined in 4.1.</div>
</div>

<p class="fontSubtitle">4.3. Stuff Segmentation Annotations</p>
<p>The stuff annotation format is identical and fully compatible to the object instance annotation format above (except iscrowd is unnecessary and set to 0). We provide annotations in json and png format for easier access, as well as several <a href="https://github.com/nightrome/coco">scripts to convert between formats</a>. Each category present in an image is encoded with a single RLE annotation (see Mask API above for more details). The category_id represents the id of the current stuff category. For more details on stuff categories and supercategories see the <a href="#stuff-eval">stuff evaluation</a> page.</p>

<p class="fontSubtitle">4.4. Image Caption Annotations</p>
<p>These annotations are used to store image captions. Each caption describes the specified image and each image has at least 5 captions (some images have more). See also the <a href="#captions-challenge2015">Captioning Challenge</a>.</p>
<div class="json">
  <div class="jsonreg">annotation{</div>
  <div class="jsonk">"id"               </div><div class="jsonv">: int,</div>
  <div class="jsonk">"image_id"         </div><div class="jsonv">: int,</div>
  <div class="jsonk">"caption"          </div><div class="jsonv">: str,</div>
  <div class="jsonreg">}</div>
</div>
