<p class="fontTitle">COCO 2017 Stuff Segmentation Challenge</p>
<p align="center"><img src="images/stuff-challenge2017.png" class="wide"/></p>

<p class="fontTitle">1. Overview</p>
<p>The COCO 2017 Stuff Segmentation Challenge is designed to push the state of the art in semantic segmentation of <i>stuff</i> classes. Whereas the <a href="#detections-challenge2017">COCO 2017 Detection Challenge</a> addresses <i>thing</i> classes (person, car, elephant), this challenge focuses on <i>stuff</i> classes (grass, wall, sky). For full details of this task please see the <a href="#stuff-eval">stuff evaluation</a> page.</p></p>
<p>Things are objects with a specific size and shape, that are often composed of parts. Stuff classes are background materials that are defined by homogeneous or repetitive patterns of fine-scale properties, but have no specific or distinctive spatial extent or shape. Why the focus on stuff? Stuff covers about 66% of the pixels in COCO. It allows us to explain important aspects of an image, including scene type; which thing classes are likely to be present and their location; as well as geometric properties of the scene. The COCO 2017 Stuff Segmentation Challenge builds on the COCO-Stuff project as described on <a href="https://github.com/nightrome/cocostuff">this website</a> and in this <a href="https://arxiv.org/abs/1612.03716">research paper</a>. This challenge includes and extends the original dataset release.  Please note that in order to scale annotation, stuff segmentations were collected on superpixel segmentations of an image.</p>
<p>This challenge is part of the <a href="https://places-coco2017.github.io/">Joint COCO and Places Recognition Challenge Workshop</a> at ICCV 2017. For further details about the joint workshop please visit the workshop website. Please also see the concurrent COCO 2017 <a href="#detections-challenge2017">Detection</a> and <a href="#keypoints-challenge2017">Keypoint</a> Challenges.</p>
<p>The challenge includes 65K COCO images (train 40K, val 5K, test-dev 5K, test-challenge 15K) with annotations for 91 stuff classes and 1 'other' class. The stuff annotations cover 45M superpixels (12B pixels) with 354K stuff regions (5.4 stuff labels per image). <b>Annotations for train and val are now available for</b> <a href="#download">download</a>, while test set annotations will remain private. We provide annotations in <b>json and png</b> format for easier access.</p>

<p class="fontTitle">2. Dates</p>
<div class="json">
  <div class="jsonktxt fontBlue">October 8, 2017</div><div class="jsonvtxt">Submission deadline (11:59 PST)</div>
  <div class="jsonktxt">October 15, 2017</div><div class="jsonvtxt">Challenge winners notified</div>
  <div class="jsonktxt">October 29, 2017</div><div class="jsonvtxt">Winners present at ICCV 2017 Workshop</div>
</div>

<p class="fontTitle">3. Organizers</p>
<div>Holger Caesar (Edinburgh)</div>
<div>Jasper Uijlings (Google)</div>
<div>Michael Maire (TTI Chicago)</div>
<div>Tsung-Yi Lin (Cornell)</div>
<div>Piotr Doll√°r (Facebook AI Research)</div>
<div>Vittorio Ferrari (Google, Edinburgh)</div>

<p class="fontTitle">4. Award Committee</p>
<div>Holger Caesar (Edinburgh)</div>
<div>Jasper Uijlings (Google)</div>
<div>Michael Maire (TTI Chicago)</div>
<div>Tsung-Yi Lin (Cornell)</div>
<div>Vittorio Ferrari (Google, Edinburgh)</div>

<p class="fontTitle">5. Challenge Guidelines</p>
<p>The <a href="#stuff-eval">stuff evaluation</a> page lists detailed information regarding how submissions will be scored and gives instructions for submitting results to the <a href="https://competitions.codalab.org/competitions/17415" target="_blank">evaluation server</a>. The test set is divided into two splits: test-dev and test-challenge. Test-dev is as the default test set for testing under general circumstances and is used to maintain a public <a href="#stuff-leaderboard">leaderboard</a> that is updated upon submission. Test-challenge is used for the workshop competition; results will be revealed during the workshop at ICCV 2017. Please note that the COCO Stuff Challenge only uses a <i>subset of the train and test sets in COCO</i>. Competitors are recommended but not restricted to train their algorithms on COCO 2017 train and val sets. The <a href="#download">download</a> page has links to all 2017 data. Please specify any and all external data used for training in the "method description" when uploading results to the evaluation server. A more thorough explanation of all these details is available on the <a href="#guidelines">guidelines</a> page, please be sure to review it carefully prior to participating. By the challenge deadline, results must be submitted to the <a href="https://competitions.codalab.org/competitions/17415" target="_blank">stuff evaluation server</a>. Competitors' algorithms will be evaluated according to the metrics described on the <a href="#stuff-eval">evaluation</a> page. Challenge participants with the most successful and innovative methods will be invited to present.</p>

<p class="fontTitle">6. Tools and Instructions</p>
<p>We provide extensive API support for the COCO images, annotations, and evaluation code. To download the COCO Stuff API, please visit our <a href="https://github.com/nightrome/coco">GitHub repository</a>. For an overview of how to use the API, please visit the <a href="#download">download</a> and <a href="#stuff-eval">stuff evaluation</a> pages. Please note that this code is currently not part of the main COCO API repository.</p>
<p>Due to the large size of COCO and the complexity of this challenge, the process of competing in this challenge may not seem simple. To help, we provide explanations and instructions for each step of the process on the <a href="#download">download</a>, <a href="#format">format</a>, <a href="#guidelines">guidelines</a> and <a href="#stuff-eval">evaluation</a> pages. For additional questions, please contact <a href="mailto:cocodataset@outlook.com">cocodataset@outlook.com</a>.</p>
